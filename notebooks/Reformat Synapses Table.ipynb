{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0d96cc",
   "metadata": {},
   "source": [
    "This script is written for loading synapses table to CAVE database. \n",
    "The script includes steps followed:\n",
    "- (1) Load csv tables from the Cloud Storage bucket of export-project\n",
    "- (2) Reformat the table to fit synapses schema\n",
    "- (3) Write the table to the Cloud Storage bucket of import-project\n",
    "\n",
    "This script excludes steps followed:\n",
    "- (0) Export required columns to the Cloud Storage bucket of export-project from the BigQuery synapses table in export-project\n",
    "- (4) Import tables from the Cloud Storage bucket of import-project to Cloud SQL of import-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from geoalchemy2 import WKBElement\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_bucket_name = 'jinhan-test'\n",
    "export_folder_path = 'synapse-export'\n",
    "export_key_path = 'lcht-goog-connectomics-xxxxx.json'\n",
    "export_project_id = \"lcht-goog-connectomics\"\n",
    "\n",
    "import_bucket_name = \"jinhan-synapse-import\"\n",
    "import_folder_path = \"synapse-import-july-2023\"\n",
    "import_key_path = 'lcht-goog-cave-temp-xxxxx.json'\n",
    "import_project_id = \"lcht-goog-cave-temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b68ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Storage Interface\n",
    "def get_bucket_file_names():\n",
    "    client = storage.Client.from_service_account_json(param.export_key_path)\n",
    "    bucket = storage.Bucket(client, param.export_bucket_name)\n",
    "    blobs = client.list_blobs(bucket, prefix=param.export_folder_path)\n",
    "    csv_files = [blob.name for blob in blobs if blob.name.lower().endswith('.csv')]\n",
    "\n",
    "    return csv_files\n",
    "\n",
    "def write_files_to_bucket(df):\n",
    "    # create Google Cloud Storage file system instance\n",
    "    fs = gcsfs.GCSFileSystem(project=param.import_project_id, token=param.import_key_path)\n",
    "\n",
    "    csv_data = df.to_csv(header=False, mode='a', index=False)\n",
    "\n",
    "    created_time_stamp = datetime.datetime.utcnow()\n",
    "\n",
    "    path = f'formatted_synapse_annotations_{created_time_stamp}.csv'\n",
    "\n",
    "    with fs.open(f'{param.import_bucket_name}/{param.import_folder_path}/{path}', 'wb') as file:\n",
    "        file.write(csv_data.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat\n",
    "def create_wkt_element(geom):\n",
    "    return WKBElement(geom.wkb)\n",
    "\n",
    "\n",
    "def format_points(data):\n",
    "    li = data.strip('[]').split()\n",
    "    data_final = [int(item) for item in li]\n",
    "    return data_final\n",
    "\n",
    "\n",
    "def process_chunk(chunk, last_index):\n",
    "    # keys in chunk dictionary are for synapses schema\n",
    "    chunk['id'] = chunk.index + last_index + 1\n",
    "    chunk['id'] = chunk['id'].astype(int)\n",
    "    chunk['created'] = datetime.datetime.utcnow()\n",
    "    chunk['deleted'] = ''\n",
    "    chunk[\"superceded_id\"] = ''\n",
    "    chunk['valid'] = 1\n",
    "    chunk['pre_pt_position'] = chunk[['pre_pt_x', 'pre_pt_y', 'pre_pt_z']].values.tolist()\n",
    "    chunk = chunk.drop(['pre_pt_x', 'pre_pt_y', 'pre_pt_z'], axis=1)\n",
    "    chunk['post_pt_position'] = chunk[['post_pt_x', 'post_pt_y', 'post_pt_z']].values.tolist()\n",
    "    chunk = chunk.drop(['post_pt_x', 'post_pt_y', 'post_pt_z'], axis=1)\n",
    "    chunk['ctr_pt_position'] = chunk[['x', 'y', 'z']].values.tolist()\n",
    "    chunk = chunk.drop(['x', 'y', 'z'], axis=1)\n",
    "    chunk['ctr_pt_position'] = chunk.ctr_pt_position.apply(Point)\n",
    "    chunk['post_pt_position'] = chunk.post_pt_position.apply(Point)\n",
    "    chunk['pre_pt_position'] = chunk.pre_pt_position.apply(Point)\n",
    "\n",
    "    chunk['pre_pt_position'] = chunk['pre_pt_position'].apply(create_wkt_element)\n",
    "    chunk['ctr_pt_position'] = chunk['ctr_pt_position'].apply(create_wkt_element)\n",
    "    chunk['post_pt_position'] = chunk['post_pt_position'].apply(create_wkt_element)\n",
    "    chunk['size'] = None\n",
    "    chunk = chunk.reindex(\n",
    "        ['id','created','deleted','superceded_id','valid','pre_pt_position','post_pt_position','ctr_pt_position', 'size'], axis=1)\n",
    "    return chunk\n",
    "\n",
    "def reformat_write(file_path, last_index):\n",
    "    fs = gcsfs.GCSFileSystem(project=param.export_project_id, token=param.export_key_path)\n",
    "\n",
    "    with fs.open(f'{param.export_bucket_name}/{file_path}', 'rb') as file:\n",
    "        chunks = pd.read_csv(file, chunksize=10000) # max size for uploading to CAVE is 10k\n",
    "\n",
    "        formatted_chunk_list = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # print(f\"Chunk {str(i)} / len({len(file_path)})\")\n",
    "            formatted_chunk = process_chunk(chunk, last_index)\n",
    "            formatted_chunk_list.append(formatted_chunk)\n",
    "        \n",
    "        df_concat = pd.concat(formatted_chunk_list)\n",
    "        write_files_to_bucket(df_concat)\n",
    "\n",
    "        last_index = last_index + df_concat.shape[0] # last_index should be cumulative to prevent duplicate ids.\n",
    "\n",
    "    return last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1558c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    file_paths = get_bucket_file_names()\n",
    "\n",
    "    last_index = 0 \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        print(f\"# {str(i)} / len({len(file_paths)}): {file_path}, {last_index}\")\n",
    "        if i > 1:\n",
    "            last_index = reformat_write(file_path, last_index)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552adc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
